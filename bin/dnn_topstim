#! /usr/bin/env python

"""
For maximizing activation of a specific layer and channel in DNN network,
selecting the topK stimuli from a stimulus set. Meanwhile, save their activation.
"""

import os
import copy
import argparse
import numpy as np

from os.path import join as pjoin
from dnnbrain.dnn.base import array_statistic
from dnnbrain.dnn.core import Stimulus
from dnnbrain.utils.util import gen_dmask
from dnnbrain.dnn import models as db_models  # Use eval to import DNN model


def main():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('-net',
                        metavar='Net',
                        type=str, required=True,
                        help='Name of DNN Model, which should be placed in '
                             '$DNNBRAIN_DATA/models with format *.pth.')
    parser.add_argument('-top',
                        metavar='TopNumber',
                        type=int, required=True,
                        help='Number of top stimulus. For example, assign top=5, and top 5 '
                        'image for each <layer,channel> pair will be selected.')
    parser.add_argument('-stim',
                        metavar='Stimulus',
                        type=str, required=True,
                        help='a .stim.csv file which contains stimulus information')
    parser.add_argument('-layer',
                        metavar='Layer',
                        required=True,
                        type=str,
                        nargs='+',
                        help="names of the layers used to specify where activation is extracted from "
                             "For example, 'conv1' represents the first convolution layer, and "
                             "'fc1' represents the first full connection layer.")
    parser.add_argument('-chn',
                        metavar='Channel',
                        required=True,
                        type=int,
                        nargs='+',
                        help="Channel numbers used to specify where activation is extracted from")
    parser.add_argument('-cuda',
                        action='store_true',
                        help='Use GPU or not')
    parser.add_argument('-out',
                        metavar='OutputDir',
                        type=str, required=True,
                        help='Output directory to save .stim.csv for top stimulus, '
                             'and associated .act.hd5 file.')
    args = parser.parse_args()

    # Load Neural Network Model
    dnn = eval('db_models.{}()'.format(args.net))

    # Load Stimulus (*.stim.csv)
    stim = Stimulus()
    stim.load(args.stim)

    # Load Dmask File (*.dmask.csv)
    dmask = gen_dmask(args.layer, args.chn, None)

    # Extract Activation
    activation = dnn.compute_activation(stim, dmask, cuda=args.cuda)

    # Create the Output File if Inexistent
    if not os.path.exists(args.out):
        os.makedirs(args.out)

    # Below Count the Activation to Select Top Stimulus
    for layer in activation.layers:
        activ = activation.get(layer)
        activ_top = np.zeros((args.top, *activ.shape[1:]))

        # Use Array_statistic in dnn.base to Do Max-pooling
        activ_pool = array_statistic(activ, 'max', axis=(2, 3), keepdims=False)

        # Do Sorting and Arg-sorting
        indices_top = np.argsort(-activ_pool, axis=0)[:args.top]

        for chn_idx, chn in enumerate(dmask.get(layer)['chn']):
            # set top stimuli information for each channel
            stim_top = stim[indices_top[:, chn_idx].tolist()]
            stim_top.set('value', activ_pool[indices_top[:, chn_idx], chn_idx])

            # save to .stim.csv
            out_file = '{}_chn{}_top{}.stim.csv'.format(layer, chn, args.top)
            stim_top.save(pjoin(args.out, out_file))

            # get top activation
            activ_top[:, chn_idx, :, :] = \
                activ[indices_top[:, chn_idx], chn_idx, :, :]

        # Set .act.h5 Activation File for Each Layer with Each Channel
        activation.set(layer, activ_top)

    # Set .act.h5 Activation File for All
    out_act_path = pjoin(args.out, 'top{}.act.h5'.format(args.top))
    activation.save(out_act_path)


if __name__ == '__main__':
    main()
